{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSyl0B/hXtkpoMZB2Sb09i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wlrma0108/Pytorch/blob/main/Residual_Network_End.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coaucNQMAXGn",
        "outputId": "5a3f550f-b50a-4df2-e5b5-2d940bb7ce2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "cbAaosCdAt65"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "mAfT2D89AuGY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=50\n",
        "batch_size=100\n",
        "learning_rate=0.001"
      ],
      "metadata": {
        "id": "-3oyJHybAujf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([transforms.Pad(4),transforms.RandomHorizontalFlip(),transforms.RandomCrop(32),transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "S5YZXtLcAuq-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=torchvision.datasets.CIFAR10(root='../../data/',train=True,transform=transform,download=True)\n",
        "test_dataset=torchvision.datasets.CIFAR10(root='../../data/',train=False,transform=transforms.ToTensor())\n",
        "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WESVyEUAAuxf",
        "outputId": "9918e688-f73a-4fea-fdd0-256f7489f8b1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3by3(in_channels, out_channels,stride=1):\n",
        "  return nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,stride=1, downsample=None):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.conv1=conv3by3(in_channels,out_channels,stride)\n",
        "    self.bn1=nn.BatchNorm2d(out_channels)\n",
        "    self.relu=nn.ReLU(inplace=True)\n",
        "    self.conv2=conv3by3(out_channels,out_channels)\n",
        "    self.bn2=nn.BatchNorm2d(out_channels)\n",
        "    self.downsample=downsample\n",
        "  def forward(self,x):\n",
        "    residual =x\n",
        "    out=self.conv1(x)\n",
        "    out=self.bn1(out)\n",
        "    out=self.relu(out)\n",
        "    out=self.conv2(out)\n",
        "    out=self.bn2(out)\n",
        "    if self.downsample:\n",
        "      residual = self.downsample(x)\n",
        "    out+=residual\n",
        "    out=self.relu(out)\n",
        "    return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self,block,layers,num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels=16\n",
        "    self.conv=conv3by3(3,16)\n",
        "    self.bn=nn.BatchNorm2d(16)\n",
        "    self.relu=nn.ReLU(inplace=True)\n",
        "    self.layer1=self.make_layer(block,16,layers[0])\n",
        "    self.layer2=self.make_layer(block,32,layers[1],2)\n",
        "    self.layer3=self.make_layer(block,64,layers[2],2)\n",
        "    self.avg_pool=nn.AvgPool2d(8)\n",
        "    self.fc=nn.Linear(64,num_classes)\n",
        "\n",
        "  def make_layer(self, block, out_channels,blocks, stride=1):\n",
        "    downsample=None\n",
        "    if(stride !=1)or(self.in_channels != out_channels):\n",
        "      downsample=nn.Sequential(\n",
        "          conv3by3(self.in_channels,out_channels,stride=stride),\n",
        "          nn.BatchNorm2d(out_channels))\n",
        "    layers =[]\n",
        "    layers.append(block(self.in_channels, out_channels, stride,downsample))\n",
        "    self.in_channels=out_channels\n",
        "    for i in range(1,blocks):\n",
        "      layers.append(block(out_channels,out_channels))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "       out = self.conv(x)\n",
        "       out = self.bn(out)\n",
        "       out = self.relu(out)\n",
        "       out = self.layer1(out)\n",
        "       out = self.layer2(out)\n",
        "       out = self.layer3(out)\n",
        "       out = self.avg_pool(out)\n",
        "       out = out.view(out.size(0), -1)\n",
        "       out = self.fc(out)\n",
        "       return out\n",
        "\n",
        "\n",
        "model=ResNet(ResidualBlock,[2,2,2]).to(device)\n"
      ],
      "metadata": {
        "id": "JqAHY2M4Ay0E"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def update_lr(optimizer,lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=lr\n",
        "\n",
        "total_step=len(train_loader)\n",
        "curr_lr=learning_rate\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i,(images, labels) in enumerate(train_loader):\n",
        "    images=images.to(device)\n",
        "    labels= labels.to(device)\n",
        "\n",
        "    outputs=model(images)\n",
        "    loss= criterion(outputs,labels)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if(i+1)%100 ==0:\n",
        "      print(\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\".format(epoch+1, num_epochs,i+1,total_step,loss.item()))\n",
        "\n",
        "  if (epoch+1)%20==0:\n",
        "    curr_lr/=3\n",
        "    update_lr(optimizer,curr_lr)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct=0\n",
        "    total=0\n",
        "    for images, labels in test_loader:\n",
        "      images= images.to(device)\n",
        "      labels= labels = labels.to(device)\n",
        "      outputs=model(images)\n",
        "      _, predicted=torch.max(outputs.data,1)\n",
        "      total+=labels.size(0)\n",
        "      correct+=(predicted==labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the model on the test images: {} %'.format(100* correct/total))\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), 'resnet.ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vV0EmCSAvVK",
        "outputId": "b9d17e35-2f77-4a71-81cb-8c52811de46d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/80], Step [100/500] Loss: 0.2289\n",
            "Epoch [1/80], Step [200/500] Loss: 0.3152\n",
            "Epoch [1/80], Step [300/500] Loss: 0.1606\n",
            "Epoch [1/80], Step [400/500] Loss: 0.2648\n",
            "Epoch [1/80], Step [500/500] Loss: 0.3512\n",
            "Epoch [2/80], Step [100/500] Loss: 0.2315\n",
            "Epoch [2/80], Step [200/500] Loss: 0.2381\n",
            "Epoch [2/80], Step [300/500] Loss: 0.2074\n",
            "Epoch [2/80], Step [400/500] Loss: 0.3551\n",
            "Epoch [2/80], Step [500/500] Loss: 0.2050\n",
            "Epoch [3/80], Step [100/500] Loss: 0.2643\n",
            "Epoch [3/80], Step [200/500] Loss: 0.5801\n",
            "Epoch [3/80], Step [300/500] Loss: 0.3506\n",
            "Epoch [3/80], Step [400/500] Loss: 0.2020\n",
            "Epoch [3/80], Step [500/500] Loss: 0.2978\n",
            "Epoch [4/80], Step [100/500] Loss: 0.1823\n",
            "Epoch [4/80], Step [200/500] Loss: 0.3523\n",
            "Epoch [4/80], Step [300/500] Loss: 0.2040\n",
            "Epoch [4/80], Step [400/500] Loss: 0.4600\n",
            "Epoch [4/80], Step [500/500] Loss: 0.3056\n",
            "Epoch [5/80], Step [100/500] Loss: 0.3228\n",
            "Epoch [5/80], Step [200/500] Loss: 0.2193\n",
            "Epoch [5/80], Step [300/500] Loss: 0.2582\n",
            "Epoch [5/80], Step [400/500] Loss: 0.1838\n",
            "Epoch [5/80], Step [500/500] Loss: 0.3613\n",
            "Epoch [6/80], Step [100/500] Loss: 0.5349\n",
            "Epoch [6/80], Step [200/500] Loss: 0.2300\n",
            "Epoch [6/80], Step [300/500] Loss: 0.4251\n",
            "Epoch [6/80], Step [400/500] Loss: 0.3134\n",
            "Epoch [6/80], Step [500/500] Loss: 0.3231\n",
            "Epoch [7/80], Step [100/500] Loss: 0.3566\n",
            "Epoch [7/80], Step [200/500] Loss: 0.2584\n",
            "Epoch [7/80], Step [300/500] Loss: 0.3744\n",
            "Epoch [7/80], Step [400/500] Loss: 0.1842\n",
            "Epoch [7/80], Step [500/500] Loss: 0.4216\n",
            "Epoch [8/80], Step [100/500] Loss: 0.2275\n",
            "Epoch [8/80], Step [200/500] Loss: 0.3381\n",
            "Epoch [8/80], Step [300/500] Loss: 0.4834\n",
            "Epoch [8/80], Step [400/500] Loss: 0.2173\n",
            "Epoch [8/80], Step [500/500] Loss: 0.3387\n",
            "Epoch [9/80], Step [100/500] Loss: 0.4444\n",
            "Epoch [9/80], Step [200/500] Loss: 0.3599\n",
            "Epoch [9/80], Step [300/500] Loss: 0.3331\n",
            "Epoch [9/80], Step [400/500] Loss: 0.3799\n",
            "Epoch [9/80], Step [500/500] Loss: 0.3901\n",
            "Epoch [10/80], Step [100/500] Loss: 0.3345\n",
            "Epoch [10/80], Step [200/500] Loss: 0.2352\n",
            "Epoch [10/80], Step [300/500] Loss: 0.3111\n",
            "Epoch [10/80], Step [400/500] Loss: 0.2611\n",
            "Epoch [10/80], Step [500/500] Loss: 0.3448\n",
            "Epoch [11/80], Step [100/500] Loss: 0.2638\n",
            "Epoch [11/80], Step [200/500] Loss: 0.1789\n",
            "Epoch [11/80], Step [300/500] Loss: 0.3786\n",
            "Epoch [11/80], Step [400/500] Loss: 0.3191\n",
            "Epoch [11/80], Step [500/500] Loss: 0.3112\n",
            "Epoch [12/80], Step [100/500] Loss: 0.2775\n",
            "Epoch [12/80], Step [200/500] Loss: 0.2674\n",
            "Epoch [12/80], Step [300/500] Loss: 0.2761\n",
            "Epoch [12/80], Step [400/500] Loss: 0.2049\n",
            "Epoch [12/80], Step [500/500] Loss: 0.3027\n",
            "Epoch [13/80], Step [100/500] Loss: 0.3599\n",
            "Epoch [13/80], Step [200/500] Loss: 0.1989\n",
            "Epoch [13/80], Step [300/500] Loss: 0.2984\n",
            "Epoch [13/80], Step [400/500] Loss: 0.1543\n",
            "Epoch [13/80], Step [500/500] Loss: 0.2977\n",
            "Epoch [14/80], Step [100/500] Loss: 0.1589\n",
            "Epoch [14/80], Step [200/500] Loss: 0.3187\n",
            "Epoch [14/80], Step [300/500] Loss: 0.3405\n",
            "Epoch [14/80], Step [400/500] Loss: 0.3942\n",
            "Epoch [14/80], Step [500/500] Loss: 0.4044\n",
            "Epoch [15/80], Step [100/500] Loss: 0.2476\n",
            "Epoch [15/80], Step [200/500] Loss: 0.1740\n",
            "Epoch [15/80], Step [300/500] Loss: 0.2246\n",
            "Epoch [15/80], Step [400/500] Loss: 0.2770\n",
            "Epoch [15/80], Step [500/500] Loss: 0.3669\n",
            "Epoch [16/80], Step [100/500] Loss: 0.2663\n",
            "Epoch [16/80], Step [200/500] Loss: 0.4703\n",
            "Epoch [16/80], Step [300/500] Loss: 0.2688\n",
            "Epoch [16/80], Step [400/500] Loss: 0.2260\n",
            "Epoch [16/80], Step [500/500] Loss: 0.2113\n",
            "Epoch [17/80], Step [100/500] Loss: 0.2452\n",
            "Epoch [17/80], Step [200/500] Loss: 0.4384\n",
            "Epoch [17/80], Step [300/500] Loss: 0.3599\n",
            "Epoch [17/80], Step [400/500] Loss: 0.2120\n",
            "Epoch [17/80], Step [500/500] Loss: 0.2634\n",
            "Epoch [18/80], Step [100/500] Loss: 0.1777\n",
            "Epoch [18/80], Step [200/500] Loss: 0.3253\n",
            "Epoch [18/80], Step [300/500] Loss: 0.3012\n",
            "Epoch [18/80], Step [400/500] Loss: 0.2373\n",
            "Epoch [18/80], Step [500/500] Loss: 0.3167\n",
            "Epoch [19/80], Step [100/500] Loss: 0.1954\n",
            "Epoch [19/80], Step [200/500] Loss: 0.1621\n",
            "Epoch [19/80], Step [300/500] Loss: 0.2666\n",
            "Epoch [19/80], Step [400/500] Loss: 0.4535\n",
            "Epoch [19/80], Step [500/500] Loss: 0.2196\n",
            "Epoch [20/80], Step [100/500] Loss: 0.2517\n",
            "Epoch [20/80], Step [200/500] Loss: 0.2990\n",
            "Epoch [20/80], Step [300/500] Loss: 0.3158\n",
            "Epoch [20/80], Step [400/500] Loss: 0.4160\n",
            "Epoch [20/80], Step [500/500] Loss: 0.4387\n",
            "Epoch [21/80], Step [100/500] Loss: 0.2849\n",
            "Epoch [21/80], Step [200/500] Loss: 0.3395\n",
            "Epoch [21/80], Step [300/500] Loss: 0.1869\n",
            "Epoch [21/80], Step [400/500] Loss: 0.2251\n",
            "Epoch [21/80], Step [500/500] Loss: 0.1417\n",
            "Epoch [22/80], Step [100/500] Loss: 0.0937\n",
            "Epoch [22/80], Step [200/500] Loss: 0.1851\n",
            "Epoch [22/80], Step [300/500] Loss: 0.1469\n",
            "Epoch [22/80], Step [400/500] Loss: 0.1298\n",
            "Epoch [22/80], Step [500/500] Loss: 0.2293\n",
            "Epoch [23/80], Step [100/500] Loss: 0.1185\n",
            "Epoch [23/80], Step [200/500] Loss: 0.1300\n",
            "Epoch [23/80], Step [300/500] Loss: 0.1991\n",
            "Epoch [23/80], Step [400/500] Loss: 0.1456\n",
            "Epoch [23/80], Step [500/500] Loss: 0.1740\n",
            "Epoch [24/80], Step [100/500] Loss: 0.1236\n",
            "Epoch [24/80], Step [200/500] Loss: 0.2824\n",
            "Epoch [24/80], Step [300/500] Loss: 0.1793\n",
            "Epoch [24/80], Step [400/500] Loss: 0.3554\n",
            "Epoch [24/80], Step [500/500] Loss: 0.1824\n",
            "Epoch [25/80], Step [100/500] Loss: 0.1341\n",
            "Epoch [25/80], Step [200/500] Loss: 0.1462\n",
            "Epoch [25/80], Step [300/500] Loss: 0.2081\n",
            "Epoch [25/80], Step [400/500] Loss: 0.2228\n",
            "Epoch [25/80], Step [500/500] Loss: 0.1566\n",
            "Epoch [26/80], Step [100/500] Loss: 0.1635\n",
            "Epoch [26/80], Step [200/500] Loss: 0.0932\n",
            "Epoch [26/80], Step [300/500] Loss: 0.2241\n",
            "Epoch [26/80], Step [400/500] Loss: 0.1683\n",
            "Epoch [26/80], Step [500/500] Loss: 0.1088\n",
            "Epoch [27/80], Step [100/500] Loss: 0.1496\n",
            "Epoch [27/80], Step [200/500] Loss: 0.1206\n",
            "Epoch [27/80], Step [300/500] Loss: 0.1512\n",
            "Epoch [27/80], Step [400/500] Loss: 0.1198\n",
            "Epoch [27/80], Step [500/500] Loss: 0.1460\n",
            "Epoch [28/80], Step [100/500] Loss: 0.1237\n",
            "Epoch [28/80], Step [200/500] Loss: 0.1455\n",
            "Epoch [28/80], Step [300/500] Loss: 0.2527\n",
            "Epoch [28/80], Step [400/500] Loss: 0.1384\n",
            "Epoch [28/80], Step [500/500] Loss: 0.1392\n",
            "Epoch [29/80], Step [100/500] Loss: 0.1614\n",
            "Epoch [29/80], Step [200/500] Loss: 0.1547\n",
            "Epoch [29/80], Step [300/500] Loss: 0.2570\n",
            "Epoch [29/80], Step [400/500] Loss: 0.0597\n",
            "Epoch [29/80], Step [500/500] Loss: 0.2128\n",
            "Epoch [30/80], Step [100/500] Loss: 0.0861\n",
            "Epoch [30/80], Step [200/500] Loss: 0.0940\n",
            "Epoch [30/80], Step [300/500] Loss: 0.1641\n",
            "Epoch [30/80], Step [400/500] Loss: 0.1498\n",
            "Epoch [30/80], Step [500/500] Loss: 0.0935\n",
            "Epoch [31/80], Step [100/500] Loss: 0.0814\n",
            "Epoch [31/80], Step [200/500] Loss: 0.1240\n",
            "Epoch [31/80], Step [300/500] Loss: 0.1405\n",
            "Epoch [31/80], Step [400/500] Loss: 0.0818\n",
            "Epoch [31/80], Step [500/500] Loss: 0.1827\n",
            "Epoch [32/80], Step [100/500] Loss: 0.2010\n",
            "Epoch [32/80], Step [200/500] Loss: 0.0831\n",
            "Epoch [32/80], Step [300/500] Loss: 0.1736\n",
            "Epoch [32/80], Step [400/500] Loss: 0.1794\n",
            "Epoch [32/80], Step [500/500] Loss: 0.1355\n",
            "Epoch [33/80], Step [100/500] Loss: 0.2132\n",
            "Epoch [33/80], Step [200/500] Loss: 0.1947\n",
            "Epoch [33/80], Step [300/500] Loss: 0.1868\n",
            "Epoch [33/80], Step [400/500] Loss: 0.2154\n",
            "Epoch [33/80], Step [500/500] Loss: 0.1929\n",
            "Epoch [34/80], Step [100/500] Loss: 0.1325\n",
            "Epoch [34/80], Step [200/500] Loss: 0.0927\n",
            "Epoch [34/80], Step [300/500] Loss: 0.0944\n",
            "Epoch [34/80], Step [400/500] Loss: 0.1806\n",
            "Epoch [34/80], Step [500/500] Loss: 0.1568\n",
            "Epoch [35/80], Step [100/500] Loss: 0.0907\n",
            "Epoch [35/80], Step [200/500] Loss: 0.1852\n",
            "Epoch [35/80], Step [300/500] Loss: 0.1240\n",
            "Epoch [35/80], Step [400/500] Loss: 0.2046\n",
            "Epoch [35/80], Step [500/500] Loss: 0.1330\n",
            "Epoch [36/80], Step [100/500] Loss: 0.1282\n",
            "Epoch [36/80], Step [200/500] Loss: 0.1587\n",
            "Epoch [36/80], Step [300/500] Loss: 0.1842\n",
            "Epoch [36/80], Step [400/500] Loss: 0.2543\n",
            "Epoch [36/80], Step [500/500] Loss: 0.1162\n",
            "Epoch [37/80], Step [100/500] Loss: 0.0868\n",
            "Epoch [37/80], Step [200/500] Loss: 0.0654\n",
            "Epoch [37/80], Step [300/500] Loss: 0.2136\n",
            "Epoch [37/80], Step [400/500] Loss: 0.1203\n",
            "Epoch [37/80], Step [500/500] Loss: 0.2069\n",
            "Epoch [38/80], Step [100/500] Loss: 0.2184\n",
            "Epoch [38/80], Step [200/500] Loss: 0.2381\n",
            "Epoch [38/80], Step [300/500] Loss: 0.1459\n",
            "Epoch [38/80], Step [400/500] Loss: 0.1189\n",
            "Epoch [38/80], Step [500/500] Loss: 0.1874\n",
            "Epoch [39/80], Step [100/500] Loss: 0.1313\n",
            "Epoch [39/80], Step [200/500] Loss: 0.0817\n",
            "Epoch [39/80], Step [300/500] Loss: 0.2539\n",
            "Epoch [39/80], Step [400/500] Loss: 0.1396\n",
            "Epoch [39/80], Step [500/500] Loss: 0.1038\n",
            "Epoch [40/80], Step [100/500] Loss: 0.0782\n",
            "Epoch [40/80], Step [200/500] Loss: 0.1497\n",
            "Epoch [40/80], Step [300/500] Loss: 0.1122\n",
            "Epoch [40/80], Step [400/500] Loss: 0.0902\n",
            "Epoch [40/80], Step [500/500] Loss: 0.1763\n",
            "Epoch [41/80], Step [100/500] Loss: 0.2105\n",
            "Epoch [41/80], Step [200/500] Loss: 0.0869\n",
            "Epoch [41/80], Step [300/500] Loss: 0.0771\n",
            "Epoch [41/80], Step [400/500] Loss: 0.0868\n",
            "Epoch [41/80], Step [500/500] Loss: 0.0770\n",
            "Epoch [42/80], Step [100/500] Loss: 0.0370\n",
            "Epoch [42/80], Step [200/500] Loss: 0.2319\n",
            "Epoch [42/80], Step [300/500] Loss: 0.0796\n",
            "Epoch [42/80], Step [400/500] Loss: 0.1050\n",
            "Epoch [42/80], Step [500/500] Loss: 0.1576\n",
            "Epoch [43/80], Step [100/500] Loss: 0.1231\n",
            "Epoch [43/80], Step [200/500] Loss: 0.1028\n",
            "Epoch [43/80], Step [300/500] Loss: 0.1423\n",
            "Epoch [43/80], Step [400/500] Loss: 0.1521\n",
            "Epoch [43/80], Step [500/500] Loss: 0.0883\n",
            "Epoch [44/80], Step [100/500] Loss: 0.1264\n",
            "Epoch [44/80], Step [200/500] Loss: 0.1134\n",
            "Epoch [44/80], Step [300/500] Loss: 0.1213\n",
            "Epoch [44/80], Step [400/500] Loss: 0.1361\n",
            "Epoch [44/80], Step [500/500] Loss: 0.2105\n",
            "Epoch [45/80], Step [100/500] Loss: 0.0856\n",
            "Epoch [45/80], Step [200/500] Loss: 0.0850\n",
            "Epoch [45/80], Step [300/500] Loss: 0.0932\n",
            "Epoch [45/80], Step [400/500] Loss: 0.2116\n",
            "Epoch [45/80], Step [500/500] Loss: 0.1047\n",
            "Epoch [46/80], Step [100/500] Loss: 0.2375\n",
            "Epoch [46/80], Step [200/500] Loss: 0.1087\n",
            "Epoch [46/80], Step [300/500] Loss: 0.0817\n",
            "Epoch [46/80], Step [400/500] Loss: 0.0761\n",
            "Epoch [46/80], Step [500/500] Loss: 0.1367\n",
            "Epoch [47/80], Step [100/500] Loss: 0.0885\n",
            "Epoch [47/80], Step [200/500] Loss: 0.1226\n",
            "Epoch [47/80], Step [300/500] Loss: 0.1562\n",
            "Epoch [47/80], Step [400/500] Loss: 0.0939\n",
            "Epoch [47/80], Step [500/500] Loss: 0.1124\n",
            "Epoch [48/80], Step [100/500] Loss: 0.0754\n",
            "Epoch [48/80], Step [200/500] Loss: 0.0477\n",
            "Epoch [48/80], Step [300/500] Loss: 0.0813\n",
            "Epoch [48/80], Step [400/500] Loss: 0.1077\n",
            "Epoch [48/80], Step [500/500] Loss: 0.0743\n",
            "Epoch [49/80], Step [100/500] Loss: 0.0804\n",
            "Epoch [49/80], Step [200/500] Loss: 0.0444\n",
            "Epoch [49/80], Step [300/500] Loss: 0.0773\n",
            "Epoch [49/80], Step [400/500] Loss: 0.1738\n",
            "Epoch [49/80], Step [500/500] Loss: 0.1538\n",
            "Epoch [50/80], Step [100/500] Loss: 0.1122\n",
            "Epoch [50/80], Step [200/500] Loss: 0.0653\n",
            "Epoch [50/80], Step [300/500] Loss: 0.0990\n",
            "Epoch [50/80], Step [400/500] Loss: 0.1607\n",
            "Epoch [50/80], Step [500/500] Loss: 0.0818\n",
            "Epoch [51/80], Step [100/500] Loss: 0.1929\n",
            "Epoch [51/80], Step [200/500] Loss: 0.0603\n",
            "Epoch [51/80], Step [300/500] Loss: 0.0696\n",
            "Epoch [51/80], Step [400/500] Loss: 0.1286\n",
            "Epoch [51/80], Step [500/500] Loss: 0.0935\n",
            "Epoch [52/80], Step [100/500] Loss: 0.1009\n",
            "Epoch [52/80], Step [200/500] Loss: 0.0816\n",
            "Epoch [52/80], Step [300/500] Loss: 0.1292\n",
            "Epoch [52/80], Step [400/500] Loss: 0.0649\n",
            "Epoch [52/80], Step [500/500] Loss: 0.1251\n",
            "Epoch [53/80], Step [100/500] Loss: 0.1271\n",
            "Epoch [53/80], Step [200/500] Loss: 0.1090\n",
            "Epoch [53/80], Step [300/500] Loss: 0.0752\n",
            "Epoch [53/80], Step [400/500] Loss: 0.1969\n",
            "Epoch [53/80], Step [500/500] Loss: 0.0910\n",
            "Epoch [54/80], Step [100/500] Loss: 0.0709\n",
            "Epoch [54/80], Step [200/500] Loss: 0.0625\n",
            "Epoch [54/80], Step [300/500] Loss: 0.1355\n",
            "Epoch [54/80], Step [400/500] Loss: 0.0782\n",
            "Epoch [54/80], Step [500/500] Loss: 0.1095\n",
            "Epoch [55/80], Step [100/500] Loss: 0.0536\n",
            "Epoch [55/80], Step [200/500] Loss: 0.0496\n",
            "Epoch [55/80], Step [300/500] Loss: 0.1787\n",
            "Epoch [55/80], Step [400/500] Loss: 0.0591\n",
            "Epoch [55/80], Step [500/500] Loss: 0.0519\n",
            "Epoch [56/80], Step [100/500] Loss: 0.1179\n",
            "Epoch [56/80], Step [200/500] Loss: 0.0994\n",
            "Epoch [56/80], Step [300/500] Loss: 0.0839\n",
            "Epoch [56/80], Step [400/500] Loss: 0.1774\n",
            "Epoch [56/80], Step [500/500] Loss: 0.0998\n",
            "Epoch [57/80], Step [100/500] Loss: 0.1226\n",
            "Epoch [57/80], Step [200/500] Loss: 0.0993\n",
            "Epoch [57/80], Step [300/500] Loss: 0.1108\n",
            "Epoch [57/80], Step [400/500] Loss: 0.0823\n",
            "Epoch [57/80], Step [500/500] Loss: 0.0540\n",
            "Epoch [58/80], Step [100/500] Loss: 0.1356\n",
            "Epoch [58/80], Step [200/500] Loss: 0.0564\n",
            "Epoch [58/80], Step [300/500] Loss: 0.0952\n",
            "Epoch [58/80], Step [400/500] Loss: 0.0452\n",
            "Epoch [58/80], Step [500/500] Loss: 0.1260\n",
            "Epoch [59/80], Step [100/500] Loss: 0.0818\n",
            "Epoch [59/80], Step [200/500] Loss: 0.1062\n",
            "Epoch [59/80], Step [300/500] Loss: 0.1580\n",
            "Epoch [59/80], Step [400/500] Loss: 0.0839\n",
            "Epoch [59/80], Step [500/500] Loss: 0.0799\n",
            "Epoch [60/80], Step [100/500] Loss: 0.0952\n",
            "Epoch [60/80], Step [200/500] Loss: 0.1347\n",
            "Epoch [60/80], Step [300/500] Loss: 0.0865\n",
            "Epoch [60/80], Step [400/500] Loss: 0.1412\n",
            "Epoch [60/80], Step [500/500] Loss: 0.0443\n",
            "Epoch [61/80], Step [100/500] Loss: 0.0833\n",
            "Epoch [61/80], Step [200/500] Loss: 0.0639\n",
            "Epoch [61/80], Step [300/500] Loss: 0.0517\n",
            "Epoch [61/80], Step [400/500] Loss: 0.0259\n",
            "Epoch [61/80], Step [500/500] Loss: 0.0596\n",
            "Epoch [62/80], Step [100/500] Loss: 0.1273\n",
            "Epoch [62/80], Step [200/500] Loss: 0.1397\n",
            "Epoch [62/80], Step [300/500] Loss: 0.2163\n",
            "Epoch [62/80], Step [400/500] Loss: 0.1410\n",
            "Epoch [62/80], Step [500/500] Loss: 0.0561\n",
            "Epoch [63/80], Step [100/500] Loss: 0.1144\n",
            "Epoch [63/80], Step [200/500] Loss: 0.0117\n",
            "Epoch [63/80], Step [300/500] Loss: 0.1507\n",
            "Epoch [63/80], Step [400/500] Loss: 0.0360\n",
            "Epoch [63/80], Step [500/500] Loss: 0.0375\n",
            "Epoch [64/80], Step [100/500] Loss: 0.0868\n",
            "Epoch [64/80], Step [200/500] Loss: 0.0211\n",
            "Epoch [64/80], Step [300/500] Loss: 0.1434\n",
            "Epoch [64/80], Step [400/500] Loss: 0.0767\n",
            "Epoch [64/80], Step [500/500] Loss: 0.0192\n",
            "Epoch [65/80], Step [100/500] Loss: 0.0893\n",
            "Epoch [65/80], Step [200/500] Loss: 0.1058\n",
            "Epoch [65/80], Step [300/500] Loss: 0.0747\n",
            "Epoch [65/80], Step [400/500] Loss: 0.0709\n",
            "Epoch [65/80], Step [500/500] Loss: 0.0974\n",
            "Epoch [66/80], Step [100/500] Loss: 0.0710\n",
            "Epoch [66/80], Step [200/500] Loss: 0.0484\n",
            "Epoch [66/80], Step [300/500] Loss: 0.0672\n",
            "Epoch [66/80], Step [400/500] Loss: 0.1251\n",
            "Epoch [66/80], Step [500/500] Loss: 0.0440\n",
            "Epoch [67/80], Step [100/500] Loss: 0.0596\n",
            "Epoch [67/80], Step [200/500] Loss: 0.1306\n",
            "Epoch [67/80], Step [300/500] Loss: 0.1163\n",
            "Epoch [67/80], Step [400/500] Loss: 0.0341\n",
            "Epoch [67/80], Step [500/500] Loss: 0.0669\n",
            "Epoch [68/80], Step [100/500] Loss: 0.1135\n",
            "Epoch [68/80], Step [200/500] Loss: 0.0823\n",
            "Epoch [68/80], Step [300/500] Loss: 0.0826\n",
            "Epoch [68/80], Step [400/500] Loss: 0.0736\n",
            "Epoch [68/80], Step [500/500] Loss: 0.0523\n",
            "Epoch [69/80], Step [100/500] Loss: 0.0314\n",
            "Epoch [69/80], Step [200/500] Loss: 0.0900\n",
            "Epoch [69/80], Step [300/500] Loss: 0.0400\n",
            "Epoch [69/80], Step [400/500] Loss: 0.0584\n",
            "Epoch [69/80], Step [500/500] Loss: 0.0852\n",
            "Epoch [70/80], Step [100/500] Loss: 0.1068\n",
            "Epoch [70/80], Step [200/500] Loss: 0.1108\n",
            "Epoch [70/80], Step [300/500] Loss: 0.0285\n",
            "Epoch [70/80], Step [400/500] Loss: 0.0512\n",
            "Epoch [70/80], Step [500/500] Loss: 0.1618\n",
            "Epoch [71/80], Step [100/500] Loss: 0.0179\n",
            "Epoch [71/80], Step [200/500] Loss: 0.0678\n",
            "Epoch [71/80], Step [300/500] Loss: 0.0841\n",
            "Epoch [71/80], Step [400/500] Loss: 0.0427\n",
            "Epoch [71/80], Step [500/500] Loss: 0.0597\n",
            "Epoch [72/80], Step [100/500] Loss: 0.1233\n",
            "Epoch [72/80], Step [200/500] Loss: 0.0956\n",
            "Epoch [72/80], Step [300/500] Loss: 0.0847\n",
            "Epoch [72/80], Step [400/500] Loss: 0.0748\n",
            "Epoch [72/80], Step [500/500] Loss: 0.1115\n",
            "Epoch [73/80], Step [100/500] Loss: 0.0409\n",
            "Epoch [73/80], Step [200/500] Loss: 0.0752\n",
            "Epoch [73/80], Step [300/500] Loss: 0.1028\n",
            "Epoch [73/80], Step [400/500] Loss: 0.0828\n",
            "Epoch [73/80], Step [500/500] Loss: 0.0689\n",
            "Epoch [74/80], Step [100/500] Loss: 0.1049\n",
            "Epoch [74/80], Step [200/500] Loss: 0.0669\n",
            "Epoch [74/80], Step [300/500] Loss: 0.0845\n",
            "Epoch [74/80], Step [400/500] Loss: 0.1239\n",
            "Epoch [74/80], Step [500/500] Loss: 0.0818\n",
            "Epoch [75/80], Step [100/500] Loss: 0.1296\n",
            "Epoch [75/80], Step [200/500] Loss: 0.2124\n",
            "Epoch [75/80], Step [300/500] Loss: 0.0947\n",
            "Epoch [75/80], Step [400/500] Loss: 0.0410\n",
            "Epoch [75/80], Step [500/500] Loss: 0.0527\n",
            "Epoch [76/80], Step [100/500] Loss: 0.0792\n",
            "Epoch [76/80], Step [200/500] Loss: 0.0709\n",
            "Epoch [76/80], Step [300/500] Loss: 0.1702\n",
            "Epoch [76/80], Step [400/500] Loss: 0.1091\n",
            "Epoch [76/80], Step [500/500] Loss: 0.1140\n",
            "Epoch [77/80], Step [100/500] Loss: 0.0806\n",
            "Epoch [77/80], Step [200/500] Loss: 0.0900\n",
            "Epoch [77/80], Step [300/500] Loss: 0.0661\n",
            "Epoch [77/80], Step [400/500] Loss: 0.1125\n",
            "Epoch [77/80], Step [500/500] Loss: 0.0373\n",
            "Epoch [78/80], Step [100/500] Loss: 0.0709\n",
            "Epoch [78/80], Step [200/500] Loss: 0.0386\n",
            "Epoch [78/80], Step [300/500] Loss: 0.0649\n",
            "Epoch [78/80], Step [400/500] Loss: 0.0829\n",
            "Epoch [78/80], Step [500/500] Loss: 0.0892\n",
            "Epoch [79/80], Step [100/500] Loss: 0.0693\n",
            "Epoch [79/80], Step [200/500] Loss: 0.1433\n",
            "Epoch [79/80], Step [300/500] Loss: 0.1103\n",
            "Epoch [79/80], Step [400/500] Loss: 0.0314\n",
            "Epoch [79/80], Step [500/500] Loss: 0.1026\n",
            "Epoch [80/80], Step [100/500] Loss: 0.1505\n",
            "Epoch [80/80], Step [200/500] Loss: 0.0629\n",
            "Epoch [80/80], Step [300/500] Loss: 0.0605\n",
            "Epoch [80/80], Step [400/500] Loss: 0.0469\n",
            "Epoch [80/80], Step [500/500] Loss: 0.0349\n",
            "Accuracy of the model on the test images: 88.55 %\n"
          ]
        }
      ]
    }
  ]
}