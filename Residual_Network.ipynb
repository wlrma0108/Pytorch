{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSyl0B/hXtkpoMZB2Sb09i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wlrma0108/Pytorch/blob/main/Residual_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coaucNQMAXGn",
        "outputId": "5a3f550f-b50a-4df2-e5b5-2d940bb7ce2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "cbAaosCdAt65"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "mAfT2D89AuGY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=50\n",
        "batch_size=100\n",
        "learning_rate=0.001"
      ],
      "metadata": {
        "id": "-3oyJHybAujf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([transforms.Pad(4),transforms.RandomHorizontalFlip(),transforms.RandomCrop(32),transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "S5YZXtLcAuq-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=torchvision.datasets.CIFAR10(root='../../data/',train=True,transform=transform,download=True)\n",
        "test_dataset=torchvision.datasets.CIFAR10(root='../../data/',train=False,transform=transforms.ToTensor())\n",
        "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WESVyEUAAuxf",
        "outputId": "9918e688-f73a-4fea-fdd0-256f7489f8b1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3by3(in_channels, out_channels,stride=1):\n",
        "  return nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,stride=1, downsample=None):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.conv1=conv3by3(in_channels,out_channels,stride)\n",
        "    self.bn1=nn.BatchNorm2d(out_channels)\n",
        "    self.relu=nn.ReLU(inplace=True)\n",
        "    self.conv2=conv3by3(out_channels,out_channels)\n",
        "    self.bn2=nn.BatchNorm2d(out_channels)\n",
        "    self.downsample=downsample\n",
        "  def forward(self,x):\n",
        "    residual =x\n",
        "    out=self.conv1(x)\n",
        "    out=self.bn1(out)\n",
        "    out=self.relu(out)\n",
        "    out=self.conv2(out)\n",
        "    out=self.bn2(out)\n",
        "    if self.downsample:\n",
        "      residual = self.downsample(x)\n",
        "    out+=residual\n",
        "    out=self.relu(out)\n",
        "    return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self,block,layers,num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels=16\n",
        "    self.conv=conv3by3(3,16)\n",
        "    self.bn=nn.BatchNorm2d(16)\n",
        "    self.relu=nn.ReLU(inplace=True)\n",
        "    self.layer1=self.make_layer(block,16,layers[0])\n",
        "    self.layer2=self.make_layer(block,32,layers[1],2)\n",
        "    self.layer3=self.make_layer(block,64,layers[2],2)\n",
        "    self.avg_pool=nn.AvgPool2d(8)\n",
        "    self.fc=nn.Linear(64,num_classes)\n",
        "\n",
        "  def make_layer(self, block, out_channels,blocks, stride=1):\n",
        "    downsample=None\n",
        "    if(stride !=1)or(self.in_channels != out_channels):\n",
        "      downsample=nn.Sequential(\n",
        "          conv3by3(self.in_channels,out_channels,stride=stride),\n",
        "          nn.BatchNorm2d(out_channels))\n",
        "    layers =[]\n",
        "    layers.append(block(self.in_channels, out_channels, stride,downsample))\n",
        "    self.in_channels=out_channels\n",
        "    for i in range(1,blocks):\n",
        "      layers.append(block(out_channels,out_channels))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "       out = self.conv(x)\n",
        "       out = self.bn(out)\n",
        "       out = self.relu(out)\n",
        "       out = self.layer1(out)\n",
        "       out = self.layer2(out)\n",
        "       out = self.layer3(out)\n",
        "       out = self.avg_pool(out)\n",
        "       out = out.view(out.size(0), -1)\n",
        "       out = self.fc(out)\n",
        "       return out\n",
        "\n",
        "\n",
        "model=ResNet(ResidualBlock,[2,2,2]).to(device)\n"
      ],
      "metadata": {
        "id": "JqAHY2M4Ay0E"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def update_lr(optimizer,lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=lr\n",
        "\n",
        "total_step=len(train_loader)\n",
        "curr_lr=learning_rate\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i,(images, labels) in enumerate(train_loader):\n",
        "    images=images.to(device)\n",
        "    labels= labels.to(device)\n",
        "\n",
        "    outputs=model(images)\n",
        "    loss= criterion(outputs,labels)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if(i+1)%100 ==0:\n",
        "      print(\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\".format(epoch+1, num_epochs,i+1,total_step,loss.item()))\n",
        "\n",
        "  if (epoch+1)%20==0:\n",
        "    curr_lr/=3\n",
        "    update_lr(optimizer,curr_lr)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct=0\n",
        "    total=0\n",
        "    for images, labels in test_loader:\n",
        "      images= images.to(device)\n",
        "      labels= labels = labels.to(device)\n",
        "      outputs=model(images)\n",
        "      _, predicted=torch.max(outputs.data,1)\n",
        "      total+=labels.size(0)\n",
        "      correct+=(predicted==labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the model on the test images: {} %'.format(100* correct/total))\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), 'resnet.ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6vV0EmCSAvVK",
        "outputId": "37da65e6-0b48-4431-b075-302735e84b4d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/80], Step [100/500] Loss: 1.6181\n",
            "Epoch [1/80], Step [200/500] Loss: 1.5416\n",
            "Epoch [1/80], Step [300/500] Loss: 1.2429\n",
            "Epoch [1/80], Step [400/500] Loss: 1.4483\n",
            "Epoch [1/80], Step [500/500] Loss: 1.1456\n",
            "Epoch [2/80], Step [100/500] Loss: 1.0458\n",
            "Epoch [2/80], Step [200/500] Loss: 1.0227\n",
            "Epoch [2/80], Step [300/500] Loss: 1.1606\n",
            "Epoch [2/80], Step [400/500] Loss: 0.7782\n",
            "Epoch [2/80], Step [500/500] Loss: 0.8658\n",
            "Epoch [3/80], Step [100/500] Loss: 0.9652\n",
            "Epoch [3/80], Step [200/500] Loss: 0.8553\n",
            "Epoch [3/80], Step [300/500] Loss: 0.9831\n",
            "Epoch [3/80], Step [400/500] Loss: 0.9142\n",
            "Epoch [3/80], Step [500/500] Loss: 0.7319\n",
            "Epoch [4/80], Step [100/500] Loss: 0.8939\n",
            "Epoch [4/80], Step [200/500] Loss: 0.7569\n",
            "Epoch [4/80], Step [300/500] Loss: 0.7993\n",
            "Epoch [4/80], Step [400/500] Loss: 0.7145\n",
            "Epoch [4/80], Step [500/500] Loss: 0.7800\n",
            "Epoch [5/80], Step [100/500] Loss: 0.7437\n",
            "Epoch [5/80], Step [200/500] Loss: 0.7328\n",
            "Epoch [5/80], Step [300/500] Loss: 0.7588\n",
            "Epoch [5/80], Step [400/500] Loss: 0.4912\n",
            "Epoch [5/80], Step [500/500] Loss: 0.6642\n",
            "Epoch [6/80], Step [100/500] Loss: 0.7648\n",
            "Epoch [6/80], Step [200/500] Loss: 0.7657\n",
            "Epoch [6/80], Step [300/500] Loss: 0.7154\n",
            "Epoch [6/80], Step [400/500] Loss: 0.5637\n",
            "Epoch [6/80], Step [500/500] Loss: 0.6700\n",
            "Epoch [7/80], Step [100/500] Loss: 0.5432\n",
            "Epoch [7/80], Step [200/500] Loss: 0.5838\n",
            "Epoch [7/80], Step [300/500] Loss: 0.5730\n",
            "Epoch [7/80], Step [400/500] Loss: 0.6211\n",
            "Epoch [7/80], Step [500/500] Loss: 0.6953\n",
            "Epoch [8/80], Step [100/500] Loss: 0.5084\n",
            "Epoch [8/80], Step [200/500] Loss: 0.4658\n",
            "Epoch [8/80], Step [300/500] Loss: 0.4818\n",
            "Epoch [8/80], Step [400/500] Loss: 0.5028\n",
            "Epoch [8/80], Step [500/500] Loss: 0.6219\n",
            "Epoch [9/80], Step [100/500] Loss: 0.4573\n",
            "Epoch [9/80], Step [200/500] Loss: 0.5917\n",
            "Epoch [9/80], Step [300/500] Loss: 0.5717\n",
            "Epoch [9/80], Step [400/500] Loss: 0.5121\n",
            "Epoch [9/80], Step [500/500] Loss: 0.5173\n",
            "Epoch [10/80], Step [100/500] Loss: 0.6186\n",
            "Epoch [10/80], Step [200/500] Loss: 0.4200\n",
            "Epoch [10/80], Step [300/500] Loss: 0.5317\n",
            "Epoch [10/80], Step [400/500] Loss: 0.5076\n",
            "Epoch [10/80], Step [500/500] Loss: 0.4341\n",
            "Epoch [11/80], Step [100/500] Loss: 0.4847\n",
            "Epoch [11/80], Step [200/500] Loss: 0.5247\n",
            "Epoch [11/80], Step [300/500] Loss: 0.5732\n",
            "Epoch [11/80], Step [400/500] Loss: 0.5823\n",
            "Epoch [11/80], Step [500/500] Loss: 0.4866\n",
            "Epoch [12/80], Step [100/500] Loss: 0.4490\n",
            "Epoch [12/80], Step [200/500] Loss: 0.4677\n",
            "Epoch [12/80], Step [300/500] Loss: 0.4295\n",
            "Epoch [12/80], Step [400/500] Loss: 0.4447\n",
            "Epoch [12/80], Step [500/500] Loss: 0.5692\n",
            "Epoch [13/80], Step [100/500] Loss: 0.4297\n",
            "Epoch [13/80], Step [200/500] Loss: 0.3845\n",
            "Epoch [13/80], Step [300/500] Loss: 0.3655\n",
            "Epoch [13/80], Step [400/500] Loss: 0.7584\n",
            "Epoch [13/80], Step [500/500] Loss: 0.5855\n",
            "Epoch [14/80], Step [100/500] Loss: 0.3585\n",
            "Epoch [14/80], Step [200/500] Loss: 0.4941\n",
            "Epoch [14/80], Step [300/500] Loss: 0.4740\n",
            "Epoch [14/80], Step [400/500] Loss: 0.5006\n",
            "Epoch [14/80], Step [500/500] Loss: 0.6894\n",
            "Epoch [15/80], Step [100/500] Loss: 0.4307\n",
            "Epoch [15/80], Step [200/500] Loss: 0.4404\n",
            "Epoch [15/80], Step [300/500] Loss: 0.4659\n",
            "Epoch [15/80], Step [400/500] Loss: 0.4793\n",
            "Epoch [15/80], Step [500/500] Loss: 0.3830\n",
            "Epoch [16/80], Step [100/500] Loss: 0.2955\n",
            "Epoch [16/80], Step [200/500] Loss: 0.3969\n",
            "Epoch [16/80], Step [300/500] Loss: 0.4810\n",
            "Epoch [16/80], Step [400/500] Loss: 0.4714\n",
            "Epoch [16/80], Step [500/500] Loss: 0.5820\n",
            "Epoch [17/80], Step [100/500] Loss: 0.2671\n",
            "Epoch [17/80], Step [200/500] Loss: 0.2930\n",
            "Epoch [17/80], Step [300/500] Loss: 0.2966\n",
            "Epoch [17/80], Step [400/500] Loss: 0.4710\n",
            "Epoch [17/80], Step [500/500] Loss: 0.2940\n",
            "Epoch [18/80], Step [100/500] Loss: 0.4517\n",
            "Epoch [18/80], Step [200/500] Loss: 0.3724\n",
            "Epoch [18/80], Step [300/500] Loss: 0.3617\n",
            "Epoch [18/80], Step [400/500] Loss: 0.4978\n",
            "Epoch [18/80], Step [500/500] Loss: 0.4202\n",
            "Epoch [19/80], Step [100/500] Loss: 0.3546\n",
            "Epoch [19/80], Step [200/500] Loss: 0.3014\n",
            "Epoch [19/80], Step [300/500] Loss: 0.4543\n",
            "Epoch [19/80], Step [400/500] Loss: 0.3610\n",
            "Epoch [19/80], Step [500/500] Loss: 0.2968\n",
            "Epoch [20/80], Step [100/500] Loss: 0.2385\n",
            "Epoch [20/80], Step [200/500] Loss: 0.4427\n",
            "Epoch [20/80], Step [300/500] Loss: 0.3991\n",
            "Epoch [20/80], Step [400/500] Loss: 0.4844\n",
            "Epoch [20/80], Step [500/500] Loss: 0.5523\n",
            "Epoch [21/80], Step [100/500] Loss: 0.3594\n",
            "Epoch [21/80], Step [200/500] Loss: 0.3430\n",
            "Epoch [21/80], Step [300/500] Loss: 0.3563\n",
            "Epoch [21/80], Step [400/500] Loss: 0.4284\n",
            "Epoch [21/80], Step [500/500] Loss: 0.2831\n",
            "Epoch [22/80], Step [100/500] Loss: 0.3421\n",
            "Epoch [22/80], Step [200/500] Loss: 0.3461\n",
            "Epoch [22/80], Step [300/500] Loss: 0.3248\n",
            "Epoch [22/80], Step [400/500] Loss: 0.3957\n",
            "Epoch [22/80], Step [500/500] Loss: 0.2548\n",
            "Epoch [23/80], Step [100/500] Loss: 0.2778\n",
            "Epoch [23/80], Step [200/500] Loss: 0.3158\n",
            "Epoch [23/80], Step [300/500] Loss: 0.2865\n",
            "Epoch [23/80], Step [400/500] Loss: 0.3260\n",
            "Epoch [23/80], Step [500/500] Loss: 0.1484\n",
            "Epoch [24/80], Step [100/500] Loss: 0.1984\n",
            "Epoch [24/80], Step [200/500] Loss: 0.2112\n",
            "Epoch [24/80], Step [300/500] Loss: 0.3579\n",
            "Epoch [24/80], Step [400/500] Loss: 0.3710\n",
            "Epoch [24/80], Step [500/500] Loss: 0.2871\n",
            "Epoch [25/80], Step [100/500] Loss: 0.3063\n",
            "Epoch [25/80], Step [200/500] Loss: 0.2327\n",
            "Epoch [25/80], Step [300/500] Loss: 0.2911\n",
            "Epoch [25/80], Step [400/500] Loss: 0.2336\n",
            "Epoch [25/80], Step [500/500] Loss: 0.2513\n",
            "Epoch [26/80], Step [100/500] Loss: 0.2672\n",
            "Epoch [26/80], Step [200/500] Loss: 0.1500\n",
            "Epoch [26/80], Step [300/500] Loss: 0.2506\n",
            "Epoch [26/80], Step [400/500] Loss: 0.1829\n",
            "Epoch [26/80], Step [500/500] Loss: 0.2816\n",
            "Epoch [27/80], Step [100/500] Loss: 0.1755\n",
            "Epoch [27/80], Step [200/500] Loss: 0.3694\n",
            "Epoch [27/80], Step [300/500] Loss: 0.3522\n",
            "Epoch [27/80], Step [400/500] Loss: 0.1014\n",
            "Epoch [27/80], Step [500/500] Loss: 0.3788\n",
            "Epoch [28/80], Step [100/500] Loss: 0.2468\n",
            "Epoch [28/80], Step [200/500] Loss: 0.3665\n",
            "Epoch [28/80], Step [300/500] Loss: 0.2572\n",
            "Epoch [28/80], Step [400/500] Loss: 0.2553\n",
            "Epoch [28/80], Step [500/500] Loss: 0.2793\n",
            "Epoch [29/80], Step [100/500] Loss: 0.2605\n",
            "Epoch [29/80], Step [200/500] Loss: 0.3331\n",
            "Epoch [29/80], Step [300/500] Loss: 0.2247\n",
            "Epoch [29/80], Step [400/500] Loss: 0.1668\n",
            "Epoch [29/80], Step [500/500] Loss: 0.2533\n",
            "Epoch [30/80], Step [100/500] Loss: 0.2158\n",
            "Epoch [30/80], Step [200/500] Loss: 0.2567\n",
            "Epoch [30/80], Step [300/500] Loss: 0.2105\n",
            "Epoch [30/80], Step [400/500] Loss: 0.2077\n",
            "Epoch [30/80], Step [500/500] Loss: 0.3142\n",
            "Epoch [31/80], Step [100/500] Loss: 0.1630\n",
            "Epoch [31/80], Step [200/500] Loss: 0.1970\n",
            "Epoch [31/80], Step [300/500] Loss: 0.1945\n",
            "Epoch [31/80], Step [400/500] Loss: 0.2398\n",
            "Epoch [31/80], Step [500/500] Loss: 0.2695\n",
            "Epoch [32/80], Step [100/500] Loss: 0.1698\n",
            "Epoch [32/80], Step [200/500] Loss: 0.1805\n",
            "Epoch [32/80], Step [300/500] Loss: 0.2879\n",
            "Epoch [32/80], Step [400/500] Loss: 0.2509\n",
            "Epoch [32/80], Step [500/500] Loss: 0.1907\n",
            "Epoch [33/80], Step [100/500] Loss: 0.2256\n",
            "Epoch [33/80], Step [200/500] Loss: 0.1251\n",
            "Epoch [33/80], Step [300/500] Loss: 0.3445\n",
            "Epoch [33/80], Step [400/500] Loss: 0.1880\n",
            "Epoch [33/80], Step [500/500] Loss: 0.1660\n",
            "Epoch [34/80], Step [100/500] Loss: 0.2801\n",
            "Epoch [34/80], Step [200/500] Loss: 0.2238\n",
            "Epoch [34/80], Step [300/500] Loss: 0.1744\n",
            "Epoch [34/80], Step [400/500] Loss: 0.3070\n",
            "Epoch [34/80], Step [500/500] Loss: 0.1854\n",
            "Epoch [35/80], Step [100/500] Loss: 0.2064\n",
            "Epoch [35/80], Step [200/500] Loss: 0.3910\n",
            "Epoch [35/80], Step [300/500] Loss: 0.2550\n",
            "Epoch [35/80], Step [400/500] Loss: 0.2178\n",
            "Epoch [35/80], Step [500/500] Loss: 0.2384\n",
            "Epoch [36/80], Step [100/500] Loss: 0.1557\n",
            "Epoch [36/80], Step [200/500] Loss: 0.2120\n",
            "Epoch [36/80], Step [300/500] Loss: 0.2234\n",
            "Epoch [36/80], Step [400/500] Loss: 0.3385\n",
            "Epoch [36/80], Step [500/500] Loss: 0.1910\n",
            "Epoch [37/80], Step [100/500] Loss: 0.1969\n",
            "Epoch [37/80], Step [200/500] Loss: 0.0963\n",
            "Epoch [37/80], Step [300/500] Loss: 0.1589\n",
            "Epoch [37/80], Step [400/500] Loss: 0.1833\n",
            "Epoch [37/80], Step [500/500] Loss: 0.1481\n",
            "Epoch [38/80], Step [100/500] Loss: 0.2239\n",
            "Epoch [38/80], Step [200/500] Loss: 0.2455\n",
            "Epoch [38/80], Step [300/500] Loss: 0.3835\n",
            "Epoch [38/80], Step [400/500] Loss: 0.2470\n",
            "Epoch [38/80], Step [500/500] Loss: 0.2324\n",
            "Epoch [39/80], Step [100/500] Loss: 0.2020\n",
            "Epoch [39/80], Step [200/500] Loss: 0.2487\n",
            "Epoch [39/80], Step [300/500] Loss: 0.3056\n",
            "Epoch [39/80], Step [400/500] Loss: 0.1949\n",
            "Epoch [39/80], Step [500/500] Loss: 0.1331\n",
            "Epoch [40/80], Step [100/500] Loss: 0.2467\n",
            "Epoch [40/80], Step [200/500] Loss: 0.3363\n",
            "Epoch [40/80], Step [300/500] Loss: 0.1506\n",
            "Epoch [40/80], Step [400/500] Loss: 0.3811\n",
            "Epoch [40/80], Step [500/500] Loss: 0.1626\n",
            "Epoch [41/80], Step [100/500] Loss: 0.2520\n",
            "Epoch [41/80], Step [200/500] Loss: 0.1258\n",
            "Epoch [41/80], Step [300/500] Loss: 0.1150\n",
            "Epoch [41/80], Step [400/500] Loss: 0.1369\n",
            "Epoch [41/80], Step [500/500] Loss: 0.1309\n",
            "Epoch [42/80], Step [100/500] Loss: 0.1313\n",
            "Epoch [42/80], Step [200/500] Loss: 0.1285\n",
            "Epoch [42/80], Step [300/500] Loss: 0.2053\n",
            "Epoch [42/80], Step [400/500] Loss: 0.2104\n",
            "Epoch [42/80], Step [500/500] Loss: 0.1490\n",
            "Epoch [43/80], Step [100/500] Loss: 0.1965\n",
            "Epoch [43/80], Step [200/500] Loss: 0.3272\n",
            "Epoch [43/80], Step [300/500] Loss: 0.1223\n",
            "Epoch [43/80], Step [400/500] Loss: 0.1024\n",
            "Epoch [43/80], Step [500/500] Loss: 0.1513\n",
            "Epoch [44/80], Step [100/500] Loss: 0.1574\n",
            "Epoch [44/80], Step [200/500] Loss: 0.1628\n",
            "Epoch [44/80], Step [300/500] Loss: 0.1602\n",
            "Epoch [44/80], Step [400/500] Loss: 0.1037\n",
            "Epoch [44/80], Step [500/500] Loss: 0.1062\n",
            "Epoch [45/80], Step [100/500] Loss: 0.2499\n",
            "Epoch [45/80], Step [200/500] Loss: 0.1315\n",
            "Epoch [45/80], Step [300/500] Loss: 0.1911\n",
            "Epoch [45/80], Step [400/500] Loss: 0.1493\n",
            "Epoch [45/80], Step [500/500] Loss: 0.2105\n",
            "Epoch [46/80], Step [100/500] Loss: 0.1457\n",
            "Epoch [46/80], Step [200/500] Loss: 0.2941\n",
            "Epoch [46/80], Step [300/500] Loss: 0.2400\n",
            "Epoch [46/80], Step [400/500] Loss: 0.1718\n",
            "Epoch [46/80], Step [500/500] Loss: 0.1907\n",
            "Epoch [47/80], Step [100/500] Loss: 0.2290\n",
            "Epoch [47/80], Step [200/500] Loss: 0.2097\n",
            "Epoch [47/80], Step [300/500] Loss: 0.1770\n",
            "Epoch [47/80], Step [400/500] Loss: 0.1048\n",
            "Epoch [47/80], Step [500/500] Loss: 0.1835\n",
            "Epoch [48/80], Step [100/500] Loss: 0.1570\n",
            "Epoch [48/80], Step [200/500] Loss: 0.1557\n",
            "Epoch [48/80], Step [300/500] Loss: 0.1081\n",
            "Epoch [48/80], Step [400/500] Loss: 0.2302\n",
            "Epoch [48/80], Step [500/500] Loss: 0.0595\n",
            "Epoch [49/80], Step [100/500] Loss: 0.2386\n",
            "Epoch [49/80], Step [200/500] Loss: 0.1792\n",
            "Epoch [49/80], Step [300/500] Loss: 0.2382\n",
            "Epoch [49/80], Step [400/500] Loss: 0.1882\n",
            "Epoch [49/80], Step [500/500] Loss: 0.1492\n",
            "Epoch [50/80], Step [100/500] Loss: 0.1210\n",
            "Epoch [50/80], Step [200/500] Loss: 0.2225\n",
            "Epoch [50/80], Step [300/500] Loss: 0.2084\n",
            "Epoch [50/80], Step [400/500] Loss: 0.2284\n",
            "Epoch [50/80], Step [500/500] Loss: 0.1307\n",
            "Epoch [51/80], Step [100/500] Loss: 0.1551\n",
            "Epoch [51/80], Step [200/500] Loss: 0.2139\n",
            "Epoch [51/80], Step [300/500] Loss: 0.1206\n",
            "Epoch [51/80], Step [400/500] Loss: 0.0761\n",
            "Epoch [51/80], Step [500/500] Loss: 0.1614\n",
            "Epoch [52/80], Step [100/500] Loss: 0.2189\n",
            "Epoch [52/80], Step [200/500] Loss: 0.1216\n",
            "Epoch [52/80], Step [300/500] Loss: 0.2041\n",
            "Epoch [52/80], Step [400/500] Loss: 0.0670\n",
            "Epoch [52/80], Step [500/500] Loss: 0.1580\n",
            "Epoch [53/80], Step [100/500] Loss: 0.1940\n",
            "Epoch [53/80], Step [200/500] Loss: 0.1895\n",
            "Epoch [53/80], Step [300/500] Loss: 0.2064\n",
            "Epoch [53/80], Step [400/500] Loss: 0.2276\n",
            "Epoch [53/80], Step [500/500] Loss: 0.1999\n",
            "Epoch [54/80], Step [100/500] Loss: 0.1728\n",
            "Epoch [54/80], Step [200/500] Loss: 0.2186\n",
            "Epoch [54/80], Step [300/500] Loss: 0.1883\n",
            "Epoch [54/80], Step [400/500] Loss: 0.1345\n",
            "Epoch [54/80], Step [500/500] Loss: 0.0778\n",
            "Epoch [55/80], Step [100/500] Loss: 0.1491\n",
            "Epoch [55/80], Step [200/500] Loss: 0.1410\n",
            "Epoch [55/80], Step [300/500] Loss: 0.1650\n",
            "Epoch [55/80], Step [400/500] Loss: 0.3235\n",
            "Epoch [55/80], Step [500/500] Loss: 0.1753\n",
            "Epoch [56/80], Step [100/500] Loss: 0.1171\n",
            "Epoch [56/80], Step [200/500] Loss: 0.2974\n",
            "Epoch [56/80], Step [300/500] Loss: 0.1846\n",
            "Epoch [56/80], Step [400/500] Loss: 0.2622\n",
            "Epoch [56/80], Step [500/500] Loss: 0.1080\n",
            "Epoch [57/80], Step [100/500] Loss: 0.1676\n",
            "Epoch [57/80], Step [200/500] Loss: 0.1365\n",
            "Epoch [57/80], Step [300/500] Loss: 0.2603\n",
            "Epoch [57/80], Step [400/500] Loss: 0.2434\n",
            "Epoch [57/80], Step [500/500] Loss: 0.1394\n",
            "Epoch [58/80], Step [100/500] Loss: 0.1283\n",
            "Epoch [58/80], Step [200/500] Loss: 0.1063\n",
            "Epoch [58/80], Step [300/500] Loss: 0.1413\n",
            "Epoch [58/80], Step [400/500] Loss: 0.1490\n",
            "Epoch [58/80], Step [500/500] Loss: 0.1914\n",
            "Epoch [59/80], Step [100/500] Loss: 0.1768\n",
            "Epoch [59/80], Step [200/500] Loss: 0.1718\n",
            "Epoch [59/80], Step [300/500] Loss: 0.1589\n",
            "Epoch [59/80], Step [400/500] Loss: 0.1712\n",
            "Epoch [59/80], Step [500/500] Loss: 0.1371\n",
            "Epoch [60/80], Step [100/500] Loss: 0.1948\n",
            "Epoch [60/80], Step [200/500] Loss: 0.1326\n",
            "Epoch [60/80], Step [300/500] Loss: 0.1128\n",
            "Epoch [60/80], Step [400/500] Loss: 0.2476\n",
            "Epoch [60/80], Step [500/500] Loss: 0.2401\n",
            "Epoch [61/80], Step [100/500] Loss: 0.0841\n",
            "Epoch [61/80], Step [200/500] Loss: 0.1794\n",
            "Epoch [61/80], Step [300/500] Loss: 0.1309\n",
            "Epoch [61/80], Step [400/500] Loss: 0.1817\n",
            "Epoch [61/80], Step [500/500] Loss: 0.1434\n",
            "Epoch [62/80], Step [100/500] Loss: 0.1188\n",
            "Epoch [62/80], Step [200/500] Loss: 0.1237\n",
            "Epoch [62/80], Step [300/500] Loss: 0.2131\n",
            "Epoch [62/80], Step [400/500] Loss: 0.1390\n",
            "Epoch [62/80], Step [500/500] Loss: 0.1052\n",
            "Epoch [63/80], Step [100/500] Loss: 0.2202\n",
            "Epoch [63/80], Step [200/500] Loss: 0.1367\n",
            "Epoch [63/80], Step [300/500] Loss: 0.0956\n",
            "Epoch [63/80], Step [400/500] Loss: 0.0323\n",
            "Epoch [63/80], Step [500/500] Loss: 0.1067\n",
            "Epoch [64/80], Step [100/500] Loss: 0.1713\n",
            "Epoch [64/80], Step [200/500] Loss: 0.1440\n",
            "Epoch [64/80], Step [300/500] Loss: 0.1316\n",
            "Epoch [64/80], Step [400/500] Loss: 0.0614\n",
            "Epoch [64/80], Step [500/500] Loss: 0.1999\n",
            "Epoch [65/80], Step [100/500] Loss: 0.1188\n",
            "Epoch [65/80], Step [200/500] Loss: 0.1158\n",
            "Epoch [65/80], Step [300/500] Loss: 0.1366\n",
            "Epoch [65/80], Step [400/500] Loss: 0.1126\n",
            "Epoch [65/80], Step [500/500] Loss: 0.1208\n",
            "Epoch [66/80], Step [100/500] Loss: 0.2348\n",
            "Epoch [66/80], Step [200/500] Loss: 0.1399\n",
            "Epoch [66/80], Step [300/500] Loss: 0.1938\n",
            "Epoch [66/80], Step [400/500] Loss: 0.1231\n",
            "Epoch [66/80], Step [500/500] Loss: 0.1024\n",
            "Epoch [67/80], Step [100/500] Loss: 0.1805\n",
            "Epoch [67/80], Step [200/500] Loss: 0.1689\n",
            "Epoch [67/80], Step [300/500] Loss: 0.0956\n",
            "Epoch [67/80], Step [400/500] Loss: 0.1565\n",
            "Epoch [67/80], Step [500/500] Loss: 0.1132\n",
            "Epoch [68/80], Step [100/500] Loss: 0.1480\n",
            "Epoch [68/80], Step [200/500] Loss: 0.2304\n",
            "Epoch [68/80], Step [300/500] Loss: 0.0682\n",
            "Epoch [68/80], Step [400/500] Loss: 0.1785\n",
            "Epoch [68/80], Step [500/500] Loss: 0.1085\n",
            "Epoch [69/80], Step [100/500] Loss: 0.0958\n",
            "Epoch [69/80], Step [200/500] Loss: 0.1221\n",
            "Epoch [69/80], Step [300/500] Loss: 0.2031\n",
            "Epoch [69/80], Step [400/500] Loss: 0.1480\n",
            "Epoch [69/80], Step [500/500] Loss: 0.1176\n",
            "Epoch [70/80], Step [100/500] Loss: 0.0822\n",
            "Epoch [70/80], Step [200/500] Loss: 0.1065\n",
            "Epoch [70/80], Step [300/500] Loss: 0.1550\n",
            "Epoch [70/80], Step [400/500] Loss: 0.1960\n",
            "Epoch [70/80], Step [500/500] Loss: 0.1625\n",
            "Epoch [71/80], Step [100/500] Loss: 0.1986\n",
            "Epoch [71/80], Step [200/500] Loss: 0.1487\n",
            "Epoch [71/80], Step [300/500] Loss: 0.0964\n",
            "Epoch [71/80], Step [400/500] Loss: 0.1689\n",
            "Epoch [71/80], Step [500/500] Loss: 0.1340\n",
            "Epoch [72/80], Step [100/500] Loss: 0.0710\n",
            "Epoch [72/80], Step [200/500] Loss: 0.1752\n",
            "Epoch [72/80], Step [300/500] Loss: 0.0944\n",
            "Epoch [72/80], Step [400/500] Loss: 0.1761\n",
            "Epoch [72/80], Step [500/500] Loss: 0.0989\n",
            "Epoch [73/80], Step [100/500] Loss: 0.1196\n",
            "Epoch [73/80], Step [200/500] Loss: 0.1137\n",
            "Epoch [73/80], Step [300/500] Loss: 0.1002\n",
            "Epoch [73/80], Step [400/500] Loss: 0.2276\n",
            "Epoch [73/80], Step [500/500] Loss: 0.0885\n",
            "Epoch [74/80], Step [100/500] Loss: 0.3058\n",
            "Epoch [74/80], Step [200/500] Loss: 0.1793\n",
            "Epoch [74/80], Step [300/500] Loss: 0.0670\n",
            "Epoch [74/80], Step [400/500] Loss: 0.1345\n",
            "Epoch [74/80], Step [500/500] Loss: 0.2508\n",
            "Epoch [75/80], Step [100/500] Loss: 0.1618\n",
            "Epoch [75/80], Step [200/500] Loss: 0.2677\n",
            "Epoch [75/80], Step [300/500] Loss: 0.1816\n",
            "Epoch [75/80], Step [400/500] Loss: 0.1019\n",
            "Epoch [75/80], Step [500/500] Loss: 0.0898\n",
            "Epoch [76/80], Step [100/500] Loss: 0.1300\n",
            "Epoch [76/80], Step [200/500] Loss: 0.1294\n",
            "Epoch [76/80], Step [300/500] Loss: 0.1382\n",
            "Epoch [76/80], Step [400/500] Loss: 0.1231\n",
            "Epoch [76/80], Step [500/500] Loss: 0.0900\n",
            "Epoch [77/80], Step [100/500] Loss: 0.1189\n",
            "Epoch [77/80], Step [200/500] Loss: 0.2056\n",
            "Epoch [77/80], Step [300/500] Loss: 0.1934\n",
            "Epoch [77/80], Step [400/500] Loss: 0.1596\n",
            "Epoch [77/80], Step [500/500] Loss: 0.0841\n",
            "Epoch [78/80], Step [100/500] Loss: 0.1280\n",
            "Epoch [78/80], Step [200/500] Loss: 0.1132\n",
            "Epoch [78/80], Step [300/500] Loss: 0.1629\n",
            "Epoch [78/80], Step [400/500] Loss: 0.1024\n",
            "Epoch [78/80], Step [500/500] Loss: 0.2012\n",
            "Epoch [79/80], Step [100/500] Loss: 0.1281\n",
            "Epoch [79/80], Step [200/500] Loss: 0.1453\n",
            "Epoch [79/80], Step [300/500] Loss: 0.1431\n",
            "Epoch [79/80], Step [400/500] Loss: 0.1002\n",
            "Epoch [79/80], Step [500/500] Loss: 0.1758\n",
            "Epoch [80/80], Step [100/500] Loss: 0.2462\n",
            "Epoch [80/80], Step [200/500] Loss: 0.1547\n",
            "Epoch [80/80], Step [300/500] Loss: 0.1447\n",
            "Epoch [80/80], Step [400/500] Loss: 0.1325\n",
            "Epoch [80/80], Step [500/500] Loss: 0.1244\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-8f1de2c95690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mtotal\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mcorrect\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy of the model on the test images: {} %'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'item'"
          ]
        }
      ]
    }
  ]
}